{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1:\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_epub(epub_path):\n",
    "    book = epub.read_epub(epub_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for item in book.get_items():\n",
    "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "            soup = BeautifulSoup(item.content, 'html.parser')\n",
    "            text += soup.get_text() + \"\\n\\n\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example Usage\n",
    "epub_path = \"/Users/sparshpatel/Documents/Codes/codes/pythonml/slm/Percy Jackson and the Olympians 5 - The Last Olympian by Rick Riordan (z-lib.org).epub\"  # Replace with your actual file\n",
    "book_text = extract_text_from_epub(epub_path)\n",
    "print(book_text[:1000])  # Print a sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2:\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "def split_into_chunks(text, max_chunk_size=1024):\n",
    "    sentences = sent_tokenize(text)  # Split into sentences\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) <= max_chunk_size:\n",
    "            chunk += sentence + \" \"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence + \" \"\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Apply chunking\n",
    "chunks = split_into_chunks(book_text)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(chunks[10:15])  # Print first 3 chunks for verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to categorise chunks, may lead to different answers\n",
    "# import re\n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# # Download necessary NLTK data\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def clean_text(text):\n",
    "#     \"\"\"Removes excessive newlines while preserving structure and removes standalone author mentions.\"\"\"\n",
    "#     text = re.sub(r'\\n+', '\\n', text)  # Replace multiple newlines with a single one\n",
    "#     text = re.sub(r'\\bRick Riordan\\b(?!\\w)', '', text, flags=re.IGNORECASE)  # Remove only standalone \"Rick Riordan\"\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
    "#     return text\n",
    "\n",
    "# def split_into_chunks(text, max_chunk_size=1024):\n",
    "#     text = clean_text(text)  # Clean text before chunking\n",
    "#     sentences = sent_tokenize(text)  # Split into sentences\n",
    "#     chunks = []\n",
    "#     chunk = \"\"\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         if len(chunk) + len(sentence) <= max_chunk_size:\n",
    "#             chunk += sentence + \" \"\n",
    "#         else:\n",
    "#             chunks.append(chunk.strip())\n",
    "#             chunk = sentence + \" \"\n",
    "\n",
    "#     if chunk:\n",
    "#         chunks.append(chunk.strip())\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "# # Apply cleaning and chunking\n",
    "# cleaned_text = clean_text(book_text)\n",
    "# chunks = split_into_chunks(cleaned_text)\n",
    "\n",
    "# print(f\"Total chunks: {len(chunks)}\")\n",
    "# print(chunks[:5])  # Print first 5 chunks for verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3:\n",
    "chunks = [chunk for chunk in chunks if chunk.strip()]  # Remove empty or whitespace-only chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4:\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")  # Higher accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5:\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the same model we used before\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Convert text chunks into embeddings\n",
    "chunk_embeddings = model.encode(chunks, convert_to_tensor=True)\n",
    "\n",
    "print(\"Chunks successfully embedded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6:\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Encode the chunks\n",
    "chunk_embeddings = embedding_model.encode(chunks, convert_to_tensor=True)  # Returns a PyTorch tensor\n",
    "chunk_embeddings = chunk_embeddings.cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "\n",
    "# Now rebuild the FAISS index\n",
    "import faiss\n",
    "index = faiss.IndexFlatL2(chunk_embeddings.shape[1])  \n",
    "index.add(chunk_embeddings)  # No more errors!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7:\n",
    "import faiss\n",
    "index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "index.add(chunk_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8:\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import torch\n",
    "\n",
    "def find_best_chunk(question):\n",
    "    # Encode the question\n",
    "    question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute similarity with all chunks\n",
    "    similarities = cosine_similarity(question_embedding, chunk_embeddings)\n",
    "    \n",
    "    # Get the index of the most relevant chunk\n",
    "    best_chunk_idx = torch.argmax(similarities).item()\n",
    "    \n",
    "    return chunks[best_chunk_idx]  # Return the most relevant chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9:\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the Question Answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10:\n",
    "def retrieve_best_chunk(question):\n",
    "    question_embedding = embedding_model.encode([question], convert_to_numpy=True)\n",
    "    _, indices = index.search(question_embedding, 1)\n",
    "    return chunks[indices[0][0]]\n",
    "\n",
    "def answer_question(question):\n",
    "    best_chunk = retrieve_best_chunk(question)\n",
    "    result = qa_pipeline(question=question, context=best_chunk)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "question = \"Who is the god of underworld??\"\n",
    "print(answer_question(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
