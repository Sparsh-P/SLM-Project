{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Language Model (SLM) for Question Answering\n",
    "Created by Sparsh Patel, email: sparshpatel0912@gmail.com\n",
    "\n",
    "## Overview\n",
    "This project implements a Small Language Model (SLM) for answering questions based on the contents of a given book. The model extracts text from an EPUB file, preprocesses it, generates embeddings for context retrieval, and then answers user queries by retrieving the most relevant chunk of text.\n",
    "The book used here is Percy Jackson and The Last Olympian\n",
    "\n",
    "## Approach\n",
    "The implementation follows these key steps:\n",
    "1. **Extract text from an EPUB file**\n",
    "2. **Chunk the extracted text into manageable sizes**\n",
    "3. **Generate embeddings for each chunk using a sentence-transformer model**\n",
    "4. **Store embeddings in a FAISS index for efficient retrieval**\n",
    "5. **Retrieve the most relevant chunk for a given question**\n",
    "6. **Use a question-answering model to generate an answer based on the retrieved chunk**\n",
    "\n",
    "## Dependencies\n",
    "- `ebooklib`\n",
    "- `BeautifulSoup`\n",
    "- `nltk`\n",
    "- `sentence-transformers`\n",
    "- `faiss-cpu`\n",
    "- `torch`\n",
    "- `transformers`\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "The implementation involves multiple NLP models working together:\n",
    "\n",
    "Sentence-Transformers (all-mpnet-base-v2): Used for embedding text chunks and questions into vector space.\n",
    "\n",
    "FAISS Index: Efficient similarity search engine for finding the most relevant text chunk.\n",
    "\n",
    "Hugging Face QA Pipeline: Generates an answer using the retrieved chunk as context.\n",
    "\n",
    "---\n",
    "\n",
    "## Pre Processing Techniques\n",
    "\n",
    "### 1. Extract Text from EPUB\n",
    "```python\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_epub(epub_path):\n",
    "    book = epub.read_epub(epub_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for item in book.get_items():\n",
    "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "            soup = BeautifulSoup(item.content, 'html.parser')\n",
    "            text += soup.get_text() + \"\\n\\n\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example Usage\n",
    "epub_path = \"path/to/book.epub\"  # Replace with actual file path\n",
    "book_text = extract_text_from_epub(epub_path)\n",
    "print(book_text[:1000])  # Print a sample\n",
    "```\n",
    "\n",
    "### 2. Split Text into Chunks\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "def split_into_chunks(text, max_chunk_size=1024):\n",
    "    sentences = sent_tokenize(text)  # Split into sentences\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) <= max_chunk_size:\n",
    "            chunk += sentence + \" \"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence + \" \"\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Apply chunking\n",
    "chunks = split_into_chunks(book_text)\n",
    "chunks = [chunk for chunk in chunks if chunk.strip()]  # Remove empty chunks\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(chunks[:5])  # Print first 5 chunks\n",
    "```\n",
    "\n",
    "### 3. Generate Embeddings\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Convert text chunks into embeddings\n",
    "chunk_embeddings = embedding_model.encode(chunks, convert_to_tensor=True)\n",
    "chunk_embeddings = chunk_embeddings.cpu().numpy()  # Convert to NumPy\n",
    "\n",
    "print(\"Chunks successfully embedded!\")\n",
    "```\n",
    "\n",
    "### 4. Create FAISS Index for Efficient Retrieval\n",
    "```python\n",
    "import faiss\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "index.add(chunk_embeddings)\n",
    "```\n",
    "\n",
    "### 5. Retrieve Most Relevant Chunk\n",
    "```python\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import torch\n",
    "\n",
    "def retrieve_best_chunk(question):\n",
    "    question_embedding = embedding_model.encode([question], convert_to_numpy=True)\n",
    "    _, indices = index.search(question_embedding, 1)\n",
    "    return chunks[indices[0][0]]\n",
    "```\n",
    "\n",
    "### 6. Answer Questions\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a question-answering model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "def answer_question(question):\n",
    "    best_chunk = retrieve_best_chunk(question)\n",
    "    result = qa_pipeline(question=question, context=best_chunk)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "# Example\n",
    "question = \"Who is Percy's mentor?\"\n",
    "print(answer_question(question))\n",
    "```\n",
    "\n",
    "## Evaluation Methodology\n",
    "- **Accuracy**: The accuracy of the model seems to be around 60%. As this an extractive based model and not a generative one the accuracy of the answers lie in the context of the chunk they are extracted from rather than the absolute answer\n",
    "- **Efficiency**: The retrieval and inference times are quick enough and generate answers usually within 2 seconds.\n",
    "- **Chunking Strategy**: We can test different chunk sizes for best accuracy.Currently the chunk size is 1024 but can be changed accordingly to fine tune the model. As we are extracting from a book, it is necessary that we have atleast a few paragraphs together for context.\n",
    "\n",
    "## Conclusion\n",
    "This approach successfully extracts text from books, chunks them for efficient retrieval, and answers user questions using an embedding-based retrieval system combined with a QA model. The system can be further optimized by experimenting with different embedding models and fine-tuning the question-answering pipeline. I used the deepset/roberta-base-squad2 Q/A model, but if you have the resources you can use Generative Model like Mistral-7B, Llama-2, or GPT-based models which can improve the answer quality\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
